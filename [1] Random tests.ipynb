{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning data into two dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:37",
     "start_time": "2017-06-12T11:38:37.816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:38",
     "start_time": "2017-06-12T11:38:38.087Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WordSPlitterMinibatchLoader = {}\n",
    "WordSPlitterMinibatchLoader.__index = WordSPlitterMinibatchLoader\n",
    "\n",
    "data_dir = \"/Users/david/Documents/MemoryNetwork/output_lua\"\n",
    "\n",
    "function WordSPlitterMinibatchLoader.create_vocabulary(input_file,vocab_file)\n",
    "    \tprint('loading text file....')\n",
    "\tlocal rawdata\n",
    "\tlocal tot_len = 0\n",
    "\tlocal f = assert(io.open(input_file,\"r\"))\n",
    "\tlocal max_sent_len = 0\n",
    "\tlocal sent_count = 0\n",
    "\t-- Create vocabulary if it doesn't exist yet\n",
    "\tprint('creating vocabulary mapping')\n",
    "\tlocal unordered = {}\n",
    "\trawdata = f:read():lower()\n",
    "\trepeat\n",
    "\t\tsent_count = sent_count + 1\n",
    "\t\tfor k,word in pairs(rawdata:split(\" \")) do \n",
    "\t\t\tword=word:lower()\n",
    "\t\t\tif not unordered[word] then unordered[word] = true end\n",
    "\t\tend\n",
    "\t\tsent_len = #rawdata:split(\" \")\n",
    "\t\tif sent_len > max_sent_len then max_sent_len=sent_len end\n",
    "\t\ttot_len = tot_len + sent_len\n",
    "\t\trawdata = f:read()\n",
    "\tuntil not rawdata\n",
    "\tf:close()\n",
    "\t-- sort into a table (i.e. keys become 1..N)\n",
    "\tlocal ordered = {}\n",
    "\tfor word in pairs(unordered) do ordered[#ordered + 1] = word end\n",
    "\ttable.sort(ordered)\n",
    "\t-- invert `ordered` to create the char->int mapping\n",
    "\tlocal vocab_mapping = {}\n",
    "\tfor i, word in ipairs(ordered) do\n",
    "\t\tvocab_mapping[word] = i\n",
    "\tend\n",
    "\tprint('saving ' .. vocab_file)\n",
    "    torch.save(vocab_file, vocab_mapping)\n",
    "    return {sent_count,vocab_mapping,max_sent_len}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:38",
     "start_time": "2017-06-12T11:38:38.666Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function WordSPlitterMinibatchLoader.create_tensor(sent_count,vocab_mapping,max_sent_len,input_file,tensor_file)\n",
    "\tprint('putting data into tensor...')\n",
    "\tlocal data = torch.ByteTensor(sent_count,max_sent_len):zero() -- store it into 1D first, then rearrange\n",
    "\tf = assert(io.open(input_file,\"r\"))\n",
    "\tlocal currline = 1\n",
    "\t-------- Writing in the tensor file \n",
    "\trawdata = f:read()\n",
    "\trepeat\n",
    "        rawdata = rawdata:lower()\n",
    "\t\tfor k,word in pairs(rawdata:split(\" \")) do \n",
    "\t\t\tdata[{currline,k}] = vocab_mapping[word:lower()]\n",
    "\t\tend\n",
    "\t\tcurrline = currline + 1\n",
    "\t\trawdata = f:read()\n",
    "\tuntil not rawdata\n",
    "\tf:close()\n",
    "\t-- save output preprocessed files\n",
    "    print('saving ' .. tensor_file)\n",
    "    torch.save(tensor_file, data)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:39",
     "start_time": "2017-06-12T11:38:39.168Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function WordSPlitterMinibatchLoader.text_to_tensor(input_file, out_vocab_file, out_tensor_file)\n",
    "    local timer = torch.Timer()\n",
    "    res = WordSPlitterMinibatchLoader.create_vocabulary(input_file,out_vocab_file)\n",
    "    local sent_count = res[1]\n",
    "    local vocab_mapping = res[2]\n",
    "    local max_sent_len = res[3]\n",
    "    return WordSPlitterMinibatchLoader.create_tensor(sent_count,vocab_mapping,max_sent_len,input_file,out_tensor_file)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:39",
     "start_time": "2017-06-12T11:38:39.598Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loading text file....\t\n",
       "creating vocabulary mapping\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "saving /Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "putting data into tensor...\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "saving /Users/david/Documents/MemoryNetwork/output_lua/data.t7\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"/Users/david/Documents/MemoryNetwork/preprocessing/output.txt\"\n",
    "out_vocab_file = \"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\"\n",
    "out_tensor_file = \"/Users/david/Documents/MemoryNetwork/output_lua/data.t7\"\n",
    "\n",
    "WordSPlitterMinibatchLoader.text_to_tensor(input_file,out_vocab_file,out_tensor_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T16:54:00",
     "start_time": "2017-06-07T14:54:00.117Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data + building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:42",
     "start_time": "2017-06-12T11:38:42.182Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = torch.load(out_tensor_file)\n",
    "vocab = torch.load(out_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:43",
     "start_time": "2017-06-12T11:38:43.890Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 70\n",
       " 57\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data:size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T11:27:53",
     "start_time": "2017-06-09T09:27:53.475Z"
    }
   },
   "source": [
    "## Easy model - with nngraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:38:45",
     "start_time": "2017-06-12T11:38:45.451Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nngraph';\n",
    "require 'math';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:37:53",
     "start_time": "2017-06-12T11:37:53.072Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug.getregistry()[\"nn.LSTM\"] = nil\n",
    "layer, parent = torch.class('nn.LSTM', 'nn.Module')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:20:45",
     "start_time": "2017-06-12T11:20:45.841Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = 5\n",
    "rnn_size = 10\n",
    "input_size = data:size(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:20:46",
     "start_time": "2017-06-12T11:20:46.055Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i=1,L do \n",
    "    inputs = {}\n",
    "    table.insert(inputs,nn.Identity()())\n",
    "    table.insert(inputs,nn.Identity()())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:20:46",
     "start_time": "2017-06-12T11:20:46.355Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for  i = 1, L do\n",
    "    if i == 1 then\n",
    "        input_size_L = input_size\n",
    "    else\n",
    "        input_size_L = rnn_size\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T13:20:49",
     "start_time": "2017-06-12T11:20:49.635Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = torch.Tensor(10):normal(0)\n",
    "a = {}\n",
    "o = {}\n",
    "table.insert(a,nn.Identity()())\n",
    "r = nn.Reshape(2,5)(a[1])\n",
    "--s = nn.SplitTable(2)(r)\n",
    "table.insert(o,r)\n",
    "\n",
    "g = nn.gModule(a, o)\n",
    "graph.dot(g.fg, 'MLP', 'myMLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T11:36:18",
     "start_time": "2017-06-12T09:36:18.591Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0291  0.1834 -1.5732 -0.1059 -0.2625\n",
       "-0.3480  0.5151 -0.0530  0.5272 -4.0310\n",
       "[torch.DoubleTensor of size 2x5]\n",
       "\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g:forward(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-12T11:28:17",
     "start_time": "2017-06-12T09:28:17.409Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T12:22:52",
     "start_time": "2017-06-09T10:22:52.897Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--\n",
    "inp = torch.Tensor(10,20,40):normal(0)\n",
    "dim_rnn = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T12:30:56",
     "start_time": "2017-06-09T10:30:56.209Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- generating the required\n",
    "inputs = {}\n",
    "table.insert(inputs,nn.Identity()())\n",
    "i2h = nn.Linear(inp:size(3),4*dim_rnn)(inputs):annotate{name='Transition input -> hidden'}\n",
    "h2h = nn.Linear(dim_rnn,dim_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T12:51:28",
     "start_time": "2017-06-09T10:51:28.823Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = {}\n",
    "outputs = {}\n",
    "prev_h = torch.Tensor(5)\n",
    "table.insert(inputs,nn.Identity()())\n",
    "table.insert(inputs,nn.Identity()())\n",
    "i2h = nn.Linear(5,50)(inputs[1]):annotate{comment=\"This is a very cool annotation\"}\n",
    "h2h = nn.Linear(50,50)(inputs[2]):annotate{comment=\"And this one is a pretty bad one\"}\n",
    "sum = nn.CAddTable()({i2h,h2h})\n",
    "table.insert(outputs,sum)\n",
    "nimp = nn.gModule(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T12:50:39",
     "start_time": "2017-06-09T10:50:39.355Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = {torch.Tensor(5):normal(0),torch.Tensor(50):normal(0)}\n",
    "aa = nimp:forward(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T15:15:15",
     "start_time": "2017-06-08T13:15:15.718Z"
    }
   },
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T15:39:55",
     "start_time": "2017-06-08T13:39:55.038Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require \"nn\";\n",
    "require 'math';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For N = 100, D = 512, T = 100, H = 1024 and with 4 bytes per number, this comes\n",
    "out to 305MB. Note that this class doesn't own input or gradOutput, so you'll\n",
    "see a bit higher memory usage in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T17:08:49",
     "start_time": "2017-06-08T15:08:49.365Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"debug.getregistry()[\"nn.LSTM\"] = nil...\"]:81: ')' expected near '='",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"debug.getregistry()[\"nn.LSTM\"] = nil...\"]:81: ')' expected near '='"
     ]
    }
   ],
   "source": [
    "debug.getregistry()[\"nn.LSTM\"] = nil\n",
    "layer, parent = torch.class('nn.LSTM', 'nn.Module')\n",
    "\n",
    "function layer:__init(input_dim,hidden_dim)\n",
    "    parent.__init(self) \n",
    "    local D,H = input_dim, hidden_dim\n",
    "    self.input_dim, self.hidden_dim = D, H\n",
    "    self.weight = torch.Tensor(D+H,4*4)\n",
    "    self.gradWeight = torch.Tensor(D+H,4*H):zero()\n",
    "    self.bias = torch.Tensor(4*H)\n",
    "    self.gradBias = torch.Tensor(4*H):zero()\n",
    "    self.reset()\n",
    "    \n",
    "    self.cell = torch.Tensor()\n",
    "    self.gates = torch.Tensor()\n",
    "    self.buffer1 = torch.Tensor()\n",
    "    self.buffer2 = torch.Tensor()\n",
    "    self.buffer3 = torch.Tensor()\n",
    "    self.grad_a_buffer = torch.Tensor()\n",
    "    \n",
    "    self.h0 = torch.Tensor()\n",
    "    self.c0 = torch.Tensor()\n",
    "    self.remember_states = false\n",
    "    \n",
    "    self.grad_c0 = torch.Tensor()\n",
    "    self.grad_h0 = torch.Tensor()\n",
    "    self.grad_x = torch.Tensor()\n",
    "    \n",
    "    self.gradInput = {self.grad_c0, self.grad_h0, self.grad_x}\n",
    "end\n",
    "\n",
    "function layer:reset(std)\n",
    "    if not std then\n",
    "        std = 1.0 / math.sqrt(self.hidden_dim + self.input_dim)\n",
    "    end\n",
    "    self.bias:zero()\n",
    "    self.bias[{{self.hidden_dim + 1, 2*self.hidden_dim}}]:fill(1)\n",
    "    self.weight:normal(0,std)\n",
    "    return self\n",
    "end\n",
    "\n",
    "\n",
    "function layer:_unpack_input(input)\n",
    "    local c0, h0, x = nil, nil, nil \n",
    "    if torch.type(input) == \"table\" and #input == 3 then\n",
    "        c0,h0,x = unpack(input)\n",
    "    elseif torch.type(input) == \"table\" and #input == 2 then\n",
    "        h0,x = unpack(input)\n",
    "    elseif torch.isTensor(input) then\n",
    "        x = input\n",
    "    else \n",
    "        assert(false,'invalid input')\n",
    "    end\n",
    "    return c0,h0,x\n",
    "end\n",
    "\n",
    "\n",
    "function layer:_get_sizes(input, gradOutput)\n",
    "  local c0, h0, x = self:_unpack_input(input)\n",
    "  local N, T = x:size(1), x:size(2)\n",
    "  local H, D = self.hidden_dim, self.input_dim\n",
    "  check_dims(x, {N, T, D})\n",
    "  if h0 then\n",
    "    check_dims(h0, {N, H})\n",
    "  end\n",
    "  if c0 then\n",
    "    check_dims(c0, {N, H})\n",
    "  end\n",
    "  if gradOutput then\n",
    "    check_dims(gradOutput, {N, T, H})\n",
    "  end\n",
    "  return N, T, D, H\n",
    "end\n",
    "\n",
    "function layer:updateOutput(input)\n",
    "    self.recompute_backward = true \n",
    "    local c0, h0, x = self:_unpack_input(input)\n",
    "    local N, T, D, H = self:_get_sizes(input)\n",
    "    \n",
    "    self._return_grad_c0 = (c0~=nil)\n",
    "    self._return_grad_h0 = (h0~=nil)\n",
    "    \n",
    "    -- initialize the Cell component\n",
    "    if not c0 then\n",
    "        c0 = self.c0\n",
    "        if c0:nElement() == 0 or not self.remember_states then\n",
    "            c0:resize(N,H):zero()\n",
    "        elseif self.remember_states then\n",
    "            local prev_N, prev_T = self.cell:size(1), self.cell:size(2)\n",
    "            assert(prev_N==N,'barch sizes must be contant to remember states')\n",
    "            c0:copy(self.cell[{{},prev_T}])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    -- initialize the Hidden States component    \n",
    "    if not h0 then\n",
    "        h0 = self.h0\n",
    "        if h0:nElement() == 0 or not self.remember_states then\n",
    "            h0:resize(N,H):zero()\n",
    "        elseif self.rember_states then\n",
    "            local prev_N, prev_T = self.output:size(1), self.output:size(2)\n",
    "            assert(prev_N == N, \"batch sizes must be the same to remember states\")\n",
    "            h0:copy(self.output[{{},prev_T}])\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    local bias_expand = self.bias:view(1,4*H):expand(N,4*H)\n",
    "    local Wx = self.weight[{{1,D}}]\n",
    "    local Wh = self.weight[{{D+1,D+H}}]\n",
    "    \n",
    "    local h,c = self.output, self.cell\n",
    "    h:resize(N,T,H):zero()\n",
    "    c:resize(N,T,H):zero()\n",
    "    local prev_h, prev_c = h0, c0\n",
    "    self.gates:resize(N,T,4*H)\n",
    "    for t=1,T do \n",
    "        local cur_x = x[{{},t}]\n",
    "        local next_g = h[{{},t}]\n",
    "        local next_c = c[{{},t}]\n",
    "        local cur_gates = self.gates[{{},t}]\n",
    "        cur_gates:addmm(bias_expand,cur_x,Wx)\n",
    "        cur_gates:addmm(prev_h,Wh)\n",
    "        cur_gates[{{}, {1, 3 * H}}]:sigmoid()\n",
    "        cur_gates[{{}, {3 * H + 1, 4 * H}}]:tanh()\n",
    "        local i = cur_gates[{{}, {1, H}}]\n",
    "        local f = cur_gates[{{}, {H + 1, 2 * H}}]\n",
    "        local o = cur_gates[{{}, {2 * H + 1, 3 * H}}]\n",
    "        local g = cur_gates[{{}, {3 * H + 1, 4 * H}}]\n",
    "        next_h:cmul(i,g)\n",
    "        next_c:cmul(f,prev_c):add(next_h)\n",
    "        next_h:tanh(next_c):cmul(o)\n",
    "        prev_h, prev_c = next_h,next_c\n",
    "    end\n",
    "    return self.output\n",
    "end\n",
    "\n",
    "function layer:backward(input, gradOutput, scale)\n",
    "    self.recompute_backward= false\n",
    "    scale = scale or 1.0\n",
    "    assert(scale == 1.0, 'must have scale=1')\n",
    "    local c0, h0, x = self._unpack_input(input)\n",
    "    if not c0 then c0 = self.c0 end\n",
    "    if not h0 then h0 = self.h0 end\n",
    "    \n",
    "    local grad_c0, grad_h0, grad_x = self.grad_c0, self.grad_h0, self.grad_x\n",
    "    local h, c = self.output, self.cells\n",
    "    local grad_h = gradOutput\n",
    "    \n",
    "    local N, T, D, H = self:_get_sizes(input,gradOutput)\n",
    "    local Wx = self.weight[{{1,D}}]\n",
    "    local Wh = self.weight[{{D+1,D+H}]\n",
    "        \n",
    "    local grad_Wx = self.gradWeight[{{1,D}}]\n",
    "    local grad_Wh = self.gradWeight[{{D+1,D+H}}]\n",
    "    local grad_b = self.gradBias\n",
    "        \n",
    "    grad_h0:resizeAs(h0):zero()\n",
    "    grad_c0:resizeAs(c0):zero()\n",
    "    for t = T, 1, -1 do\n",
    "        local next_h, next_c = h[{{},t}], c[{{},t}]\n",
    "        local tanh_next_c2 = grad_af:cmul(tanh)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T16:39:46",
     "start_time": "2017-06-08T14:39:46.590Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c0 = torch.Tensor(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T17:10:24",
     "start_time": "2017-06-08T15:10:24.478Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T17:09:57",
     "start_time": "2017-06-08T15:09:57.150Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T11:19:59",
     "start_time": "2017-06-09T09:19:59.531Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  5\n",
       "  3\n",
       " 10\n",
       "[torch.LongStorage of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.Tensor(5,3,10)\n",
    "print(c:size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T11:23:56",
     "start_time": "2017-06-09T09:23:56.353Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "/Users/david/torch/install/share/lua/5.1/torch/Tensor.lua:462: bad argument #1 to 'set' (expecting number or torch.DoubleTensor or torch.DoubleStorage at /Users/david/torch/pkg/torch/generic/Tensor.c:1153)\nstack traceback:\n\t[C]: in function 'set'\n\t/Users/david/torch/install/share/lua/5.1/torch/Tensor.lua:462: in function 'view'\n\t[string \"a = torch.Tensor()...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:210: in function </Users/david/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t/Users/david/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010c6e4d10",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "/Users/david/torch/install/share/lua/5.1/torch/Tensor.lua:462: bad argument #1 to 'set' (expecting number or torch.DoubleTensor or torch.DoubleStorage at /Users/david/torch/pkg/torch/generic/Tensor.c:1153)\nstack traceback:\n\t[C]: in function 'set'\n\t/Users/david/torch/install/share/lua/5.1/torch/Tensor.lua:462: in function 'view'\n\t[string \"a = torch.Tensor()...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:210: in function </Users/david/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t/Users/david/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010c6e4d10"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor()\n",
    "c:view(a,10,5,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-09T11:23:39",
     "start_time": "2017-06-09T09:23:39.847Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nn.LSTM(10,100)\n",
    "a.weight:size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-08T15:36:32",
     "start_time": "2017-06-08T13:36:32.578Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 110\n",
       "  16\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'nngraph';\n",
    "GRU = {}\n",
    "\n",
    "function GRU.instanciate_model(input_size,rnn_size,n)\n",
    "    print(a)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:58",
     "start_time": "2017-06-07T11:55:58.314Z"
    }
   },
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-14T11:42:06",
     "start_time": "2017-06-14T09:42:06.526Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " -0.0000e+00\n",
       " -0.0000e+00\n",
       "  4.9046e-62\n",
       "  4.7607e-90\n",
       " 4.7478e+174\n",
       " 3.9706e+246\n",
       "  1.1632e-28\n",
       " 8.7644e+169\n",
       "  2.3278e-57\n",
       "  1.4270e-71\n",
       "[torch.DoubleTensor of size 10]\n",
       "\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       " -0.0000e+00\n",
       " -0.0000e+00\n",
       "  1.2262e-62\n",
       "  1.1902e-90\n",
       " 1.1869e+174\n",
       " 9.9266e+245\n",
       "  2.9080e-29\n",
       " 2.1911e+169\n",
       "  5.8196e-58\n",
       "  3.5674e-72\n",
       "[torch.DoubleTensor of size 10]\n",
       "\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaaa = torch.Tensor(10)\n",
    "print(aaaa)\n",
    "aaaa = aaaa:div(4)\n",
    "print(aaaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
