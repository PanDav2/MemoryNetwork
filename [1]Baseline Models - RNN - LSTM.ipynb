{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Recreating-the-original-sentence-for-test-purposes\" data-toc-modified-id=\"Recreating-the-original-sentence-for-test-purposes-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Recreating the original sentence for test purposes</a></div><div class=\"lev2 toc-item\"><a href=\"#OneHot\" data-toc-modified-id=\"OneHot-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>OneHot</a></div><div class=\"lev1 toc-item\"><a href=\"#Implementing-the-RNN-training-as-baseline\" data-toc-modified-id=\"Implementing-the-RNN-training-as-baseline-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Implementing the RNN training as baseline</a></div><div class=\"lev2 toc-item\"><a href=\"#Batch-Loader\" data-toc-modified-id=\"Batch-Loader-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Batch Loader</a></div><div class=\"lev2 toc-item\"><a href=\"#Next-Batch-Method\" data-toc-modified-id=\"Next-Batch-Method-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Next Batch Method</a></div><div class=\"lev3 toc-item\"><a href=\"#Test-for-batch-reconstruction\" data-toc-modified-id=\"Test-for-batch-reconstruction-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Test for batch reconstruction</a></div><div class=\"lev2 toc-item\"><a href=\"#Loading-the-LSTM\" data-toc-modified-id=\"Loading-the-LSTM-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Loading the LSTM</a></div><div class=\"lev3 toc-item\"><a href=\"#Drawing-the-graph\" data-toc-modified-id=\"Drawing-the-graph-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Drawing the graph</a></div><div class=\"lev2 toc-item\"><a href=\"#Initializing-LSTM\" data-toc-modified-id=\"Initializing-LSTM-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Initializing LSTM</a></div><div class=\"lev2 toc-item\"><a href=\"#Training-LSTM\" data-toc-modified-id=\"Training-LSTM-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Training LSTM</a></div><div class=\"lev3 toc-item\"><a href=\"#Defining-the-cloning-list-utils\" data-toc-modified-id=\"Defining-the-cloning-list-utils-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Defining the cloning list utils</a></div><div class=\"lev3 toc-item\"><a href=\"#Defining-the-training-protocol\" data-toc-modified-id=\"Defining-the-training-protocol-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Defining the training protocol</a></div><div class=\"lev2 toc-item\"><a href=\"#Optimization-phase\" data-toc-modified-id=\"Optimization-phase-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Optimization phase</a></div><div class=\"lev1 toc-item\"><a href=\"#Interpreting-model-training-and-parameters\" data-toc-modified-id=\"Interpreting-model-training-and-parameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Interpreting model training and parameters</a></div><div class=\"lev1 toc-item\"><a href=\"#Sampling-from-trained-model\" data-toc-modified-id=\"Sampling-from-trained-model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Sampling from trained model</a></div><div class=\"lev2 toc-item\"><a href=\"#Reconstructing-the-first-input\" data-toc-modified-id=\"Reconstructing-the-first-input-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Reconstructing the first input</a></div><div class=\"lev3 toc-item\"><a href=\"#Little-debugging\" data-toc-modified-id=\"Little-debugging-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Little debugging</a></div><div class=\"lev1 toc-item\"><a href=\"#Scrap\" data-toc-modified-id=\"Scrap-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Scrap</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:01",
     "start_time": "2017-08-28T09:27:00.921Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'nn'\n",
    "require 'nngraph'\n",
    "require 'optim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:01",
     "start_time": "2017-08-28T09:27:00.931Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = require 'utils.data_loader';\n",
    "QUICKLOAD = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:01",
     "start_time": "2017-08-28T09:27:00.938Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if QUICKLOAD then\n",
    "    x = torch.load(\"/Users/david/Documents/MemoryNetwork/output_lua/sample.t7\")\n",
    "    y = torch.load(\"/Users/david/Documents/MemoryNetwork/output_lua/label.t7\")\n",
    "    voc = torch.load(\"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\")\n",
    "    index = torch.load(\"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7_index\")\n",
    "else \n",
    "    input_file = \"/Users/david/Documents/MemoryNetwork/preprocessing/output.txt\"\n",
    "    out_vocab_file = \"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\"\n",
    "    out_tensor_file = \"/Users/david/Documents/MemoryNetwork/output_lua/data.t7\"\n",
    "    voc = torch.load(\"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\")\n",
    "    index = torch.load(\"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7_index\")\n",
    "    x, y = data_loader.text_to_tensor(input_file,out_vocab_file,out_tensor_file)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreating the original sentence for test purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:01",
     "start_time": "2017-08-28T09:27:01.931Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug.getregistry()[\"OneHot\"] = nil\n",
    "OneHot, parent = torch.class('OneHot', 'nn.Module')\n",
    "\n",
    "function OneHot:__init(outputSize)\n",
    "  parent.__init(self)\n",
    "  self.outputSize = outputSize\n",
    "  -- We'll construct one-hot encodings by using the index method to\n",
    "  -- reshuffle the rows of an identity matrix. To avoid recreating\n",
    "  -- it every iteration we'll cache it.\n",
    "  self._eye = torch.eye(outputSize)\n",
    "end\n",
    "\n",
    "function OneHot:updateOutput(input)\n",
    "  self.output:resize(input:size(1), self.outputSize):zero()\n",
    "  if self._eye == nil then self._eye = torch.eye(self.outputSize) end\n",
    "  self._eye = self._eye:float()\n",
    "  local longInput = input:long()\n",
    "  self.output:copy(self._eye:index(1, longInput))\n",
    "  return self.output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the RNN training as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:02",
     "start_time": "2017-08-28T09:27:02.173Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn';\n",
    "require 'nngraph';\n",
    "LSTM = require 'models.lstm';\n",
    "--require 'utils.OneHot';\n",
    "model_utils = require 'utils.model_utils';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:02",
     "start_time": "2017-08-28T09:27:02.177Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RNN_SIZE = 10\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0\n",
    "SEQ_LENGTH = 54\n",
    "BATCH_SIZE = 1\n",
    "VOCAB_SIZE = data_loader.count_table_elements(voc)+2\n",
    "GRAD_CLIP = 5\n",
    "LEARNING_RATE = 2e-3\n",
    "LEARNING_RATE_DECAY = 0.97\n",
    "LEARNING_RATE_DECAY_AFTER = 10\n",
    "DECAY_RATE = 0.95\n",
    "MAX_EPOCH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Batch Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:02",
     "start_time": "2017-08-28T09:27:02.442Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BatchLoader = {}\n",
    "BatchLoader.__index = BatchLoader\n",
    "\n",
    "function BatchLoader.create(x,y,batch_size,seq_length)\n",
    "    local self = {}\n",
    "    setmetatable(self,BatchLoader)\n",
    "    -- self.batches is a table of tensor\n",
    "    print('reshaping tensor...')\n",
    "    self.batch_size = batch_size\n",
    "    self.seq_length = seq_length\n",
    "    self.ix = 1\n",
    "    \n",
    "    self.x_batches = x:view(BATCH_SIZE,-1):split(SEQ_LENGTH,2) -- #rows = #batches\n",
    "    self.nbatches = #self.x_batches\n",
    "    \n",
    "    self.y_batches = y:view(BATCH_SIZE,-1):split(SEQ_LENGTH,2) -- #rows = #batches\n",
    "    self.y_nbatches = #self.y_batches\n",
    "    print(#self.x_batches)\n",
    "    print(#self.y_batches)\n",
    "    assert(#self.x_batches == #self.y_batches)\n",
    "    \n",
    "    -- lets try to be helpful here\n",
    "    if self.nbatches < 50 then\n",
    "        print('WARNING: less than 50 batches in the data in total? Looks like very small dataset. You probably want to use smaller batch_size and/or seq_length.')\n",
    "    end\n",
    "    \n",
    "    collectgarbage()\n",
    "    return self \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T13:35:00",
     "start_time": "2017-08-23T11:35:00.693Z"
    }
   },
   "source": [
    "## Next Batch Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:02",
     "start_time": "2017-08-28T09:27:02.687Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function BatchLoader:next_batch()\n",
    "    if self.nbatches < self.ix then\n",
    "        self.ix = 1 -- cycling through the batch\n",
    "    end\n",
    "    local x = self.x_batches[self.ix]\n",
    "    local y = self.y_batches[self.ix]\n",
    "    self.ix = self.ix + 1\n",
    "    return x,y \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:02",
     "start_time": "2017-08-28T09:27:02.693Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reshaping tensor...\t\n",
       "70\t\n",
       "70\t\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = BatchLoader.create(x,y,BATCH_SIZE,SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:02",
     "start_time": "2017-08-28T09:27:02.696Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 20\n",
       "  4  26  25   2  37   9  16  40  38  37   9  13  39  37  28  25  37  33  13  29\n",
       "\n",
       "Columns 21 to 40\n",
       " 37  28  25  37  12  13  27  16  25  37   9   4  40  23  37   8  38  37  37  11\n",
       "\n",
       "Columns 41 to 54\n",
       " 10  24  15  35  14  41  26  37  37  34   3   1   1   1\n",
       "[torch.ByteTensor of size 1x54]\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.x_batches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T13:29:44",
     "start_time": "2017-08-23T11:29:44.780Z"
    }
   },
   "source": [
    "### Test for batch reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-24T16:32:38",
     "start_time": "2017-08-24T14:32:38.545Z"
    },
    "collapsed": false
   },
   "source": [
    "for i=1,B.x_batches[1]:size(2) do\n",
    "    local v = B.x_batches[1][{1,i}]\n",
    "    print(index[v])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:03",
     "start_time": "2017-08-28T09:27:03.474Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSTM = require 'models.lstm';\n",
    "protos = {}\n",
    "protos.rnn = LSTM.lstm(VOCAB_SIZE, RNN_SIZE, NUM_LAYERS, DROPOUT)\n",
    "protos.criterion = nn.ClassNLLCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T11:23:30",
     "start_time": "2017-08-23T09:23:30.216Z"
    },
    "collapsed": true
   },
   "source": [
    "### Drawing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:03",
     "start_time": "2017-08-28T09:27:03.713Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- graph.dot(protos.rnn.fg, 'rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-22T14:48:39",
     "start_time": "2017-08-22T12:48:39.232Z"
    }
   },
   "source": [
    "## Initializing LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:03",
     "start_time": "2017-08-28T09:27:03.962Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function prepro(x,y)\n",
    "    local x = x:transpose(1,2):contiguous()\n",
    "    local y = y:transpose(1,2):contiguous()\n",
    "    return x,y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:03",
     "start_time": "2017-08-28T09:27:03.959Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_state = {}\n",
    "for L=1,NUM_LAYERS do\n",
    "    local h_init = torch.zeros(BATCH_SIZE, RNN_SIZE)\n",
    "    table.insert(init_state,h_init:clone())\n",
    "    table.insert(init_state,h_init:clone()) -- because LSTM\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:03",
     "start_time": "2017-08-28T09:27:03.967Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialized parameters\t\n",
       "parameters uniformed\t\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- put the above things into one flattened parameters tensor\n",
    "params, grad_params = model_utils.combine_all_parameters(protos.rnn)\n",
    "print(\"initialized parameters\")\n",
    "-- initialization\n",
    "params:uniform(-0.08,0.08) -- small uniform numbers\n",
    "print(\"parameters uniformed\")\n",
    "-- initialize the LSTM forget gates with slightly higher biases to encorage remembering in the beginning\n",
    "for layer_idx = 1, NUM_LAYERS do\n",
    "    for _, node in ipairs(protos.rnn.forwardnodes) do\n",
    "        if node.data.annotations.name  == \"i2h\".. layer_idx then\n",
    "            print('setting forget gate biases to 1 in LSTM layer '.. layer_idx) \n",
    "            -- the gates are in order i,f,o,g so f is the 2nd block of weights\n",
    "            node.data.module.bias[{{RNN_SIZE+1,2*RNN_SIZE}}]:fill(1.0)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:07",
     "start_time": "2017-08-28T09:27:03.973Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cloning rnn\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "cloning criterion\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- make a bunch of clones after flattening, as that reallocates memory\n",
    "clones = {}\n",
    "for name,proto in pairs(protos) do\n",
    "    print('cloning ' .. name)\n",
    "    clones[name] = model_utils.clone_many_times(proto,SEQ_LENGTH, not proto.parameters)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the cloning list utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:07",
     "start_time": "2017-08-28T09:27:04.833Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function clone_list(tensor_list, zeto_too)\n",
    "    -- utility function. todo : move away to some utils file?\n",
    "    -- takes a list of tensors and returns a list of cloned tensors\n",
    "    local out = {}\n",
    "    for k,v in pairs(tensor_list) do\n",
    "        out[k] = v:clone()\n",
    "        if zero_too then out[k]:zero() end\n",
    "    end\n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:07",
     "start_time": "2017-08-28T09:27:05.298Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local init_state_global = clone_list(init_state)\n",
    "function feval(opti_params)\n",
    "    --[[if opti_params ~= params then \n",
    "        params:copy(opti_params)\n",
    "    end ]]--\n",
    "    grad_params:zero()\n",
    "    ------------------ get minibatch -------------------\n",
    "    local x,y = B:next_batch()\n",
    "    local x,y = prepro(x,y)\n",
    "    ------------------ forward pass -------------------\n",
    "    local rnn_state = {[0] = init_state_global}\n",
    "    local predictions = {} --- softmax outputs\n",
    "    local loss = 0\n",
    "    for t = 1,SEQ_LENGTH do\n",
    "        -- print(\"iterations : \"..t)\n",
    "        clones.rnn[t]:training()  -- make sure we are in correct mode (this is cheap, sets flag)\n",
    "        -- print(unpack(rnn_state[t-1]))\n",
    "        local lst = clones.rnn[t]:forward{x[t],unpack(rnn_state[t-1])}\n",
    "        rnn_state[t] = {}\n",
    "        for i=1,#init_state do table.insert(rnn_state[t], lst[i]) end -- extract the state, without output\n",
    "        predictions[t] = lst[#lst] -- last element is the prediction\n",
    "        loss = loss + clones.criterion[t]:forward(predictions[t],y[t])\n",
    "    end\n",
    "    loss = loss / SEQ_LENGTH\n",
    "    ------------------ backward pass -------------------\n",
    "    -- initialize radient at time t to be zeros (there's no influence from future)\n",
    "    local drnn_state = {[SEQ_LENGTH] = clone_list(init_state,true)} -- true also zeros the clones\n",
    "    for t = SEQ_LENGTH,1,-1 do\n",
    "        -- backprop through loss, and softmax / linear\n",
    "        local doutput_t = clones.criterion[t]:backward(predictions[t],y[t])\n",
    "        table.insert(drnn_state[t],doutput_t)\n",
    "        local dlst = clones.rnn[t]:backward({x[t], unpack(rnn_state[t-1])}, drnn_state[t])\n",
    "        drnn_state[t-1] = {}\n",
    "        for k,v in pairs(dlst) do\n",
    "            if k> 1 then --k == 1 is gradient on x, which we don't need\n",
    "                -- note we do k-1 because first item is dembeddings, and then follow the\n",
    "                -- derivatives of the state, starting at index 2. \n",
    "                drnn_state[t-1][k-1] = v\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    ------------------ backward pass -------------------\n",
    "    -- transfer final state to initial state (BPTT)\n",
    "    init_state_global = rnn_state[#rnn_state] -- does this need to be a clone ?\n",
    "    -- grad_params:div(SEQ_LENGTH)\n",
    "    -- clip gradient element wise\n",
    "    grad_params:clamp(-GRAD_CLIP,GRAD_CLIP)\n",
    "    return loss, grad_params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:27:07",
     "start_time": "2017-08-28T09:27:05.588Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "-- feval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- start optimization here\n",
    "train_losses = {}\n",
    "val_losses = {}\n",
    "local optim_state = {learning_rate = LEARNING_RATE, alpha= DECAY_RATE}\n",
    "local iterations = MAX_EPOCH * B.nbatches\n",
    "local iterations_per_epoch = MAX_EPOCH*B.nbatches\n",
    "local loss0 = nil\n",
    "\n",
    "-- Optimization starts here\n",
    "for i =1, iterations do\n",
    "    local epoch = i / B.nbatches\n",
    "    local timer = torch.Timer()\n",
    "    local _, loss = optim.rmsprop(feval,params,optim_state)\n",
    "    local train_loss = loss[1] -- the loss is inside a list, pop it\n",
    "    train_losses[i] = train_loss\n",
    "    print(train_losses[i])\n",
    "    -- exponential learning rate decay\n",
    "    if i % B.nbatches == 0 and LEARNING_RATE_DECAY < 1 then\n",
    "        if epoch >= LEARNING_RATE_DECAY_AFTER then\n",
    "            local decay_factor = LEARNING_RATE_DECAY\n",
    "            optim_state.learning_rate = optim_state.learning_rate * decay_factor -- decay it\n",
    "            print('decayed learning rate by a factor ' .. decay_factor .. ' to ' .. optim_state.learning_rate)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    -- every now and then or on last iteration\n",
    "    if i % 10 == 0 then collectgarbage() end\n",
    "    \n",
    "    if loss[1] ~= loss[1] then\n",
    "        print('loss is NaN.  This usually indicates a bug.  Please check the issues page for existing issues, or create a new issue, if none exist.  Ideally, please state: your operating system, 32-bit/64-bit, your blas version, cpu/cuda/cl?')\n",
    "        break -- halt\n",
    "    end\n",
    "    \n",
    "    if loss0 == nil then loss0 = loss[1] end\n",
    "    if loss[1] > loss0 * 3 then\n",
    "        print('loss is exploding, aborting.')\n",
    "        break -- halt\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:28:37",
     "start_time": "2017-08-28T09:28:37.523Z"
    },
    "collapsed": false
   },
   "source": [
    "# Interpreting model training and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing the first input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T12:24:53",
     "start_time": "2017-08-28T10:24:52.551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function tensor_to_table(tens,ind)\n",
    "    ind = ind or 1\n",
    "    local sentence = {}\n",
    "    for i=1,tens:size(2) do\n",
    "        table.insert(sentence,index[tens[{ind,i}]])\n",
    "    end\n",
    "    return sentence\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function table_to_string(sentence)\n",
    "    local s = ''\n",
    "    for k,w in pairs(sentence) do\n",
    "        s = s..' '.. w\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T12:26:03",
     "start_time": "2017-08-28T10:26:03.299Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51\t\n",
       " ashley is in  the bathroom. cassidy went to the bathroom. billy took the lamp in the porch. billy left the lamp in the bedsit. billy joined cassidy in the bathroom. ashley went from the bathroom to the the bedsit because he can't stand billy. where is the the radio ?\t\n"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local tt = tensor_to_table(x)\n",
    "local ttt = table_to_string(tt)\n",
    "print(#tt)\n",
    "print(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T12:10:05",
     "start_time": "2017-08-28T10:10:05.276Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEMP_SAMPLING = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T12:41:33",
     "start_time": "2017-08-28T10:41:33.138Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T12:52:30",
     "start_time": "2017-08-28T10:52:24.379Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-----------------------\t\n",
       "\"ashley\"\t\n",
       "\"is\"\t\n",
       "\"in\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"\"\t\n",
       "\"the\"\t\n",
       "\"bathroom.\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"cassidy\"\t\n",
       "\"went\"\t\n",
       "\"to\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"the\"\t\n",
       "\"bathroom.\"\t\n",
       "\"billy\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"took\"\t\n",
       "\"the\"\t\n",
       "\"lamp\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"in\"\t\n",
       "\"the\"\t\n",
       "\"porch.\"\t\n",
       "\"billy\"\t\n",
       "\"left\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"the\"\t\n",
       "\"lamp\"\t\n",
       "\"in\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"the\"\t\n",
       "\"bedsit.\"\t\n",
       "\"billy\"\t\n",
       "\"joined\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"cassidy\"\t\n",
       "\"in\"\t\n",
       "\"the\"\t\n",
       "\"bathroom.\"\t\n",
       "\"ashley\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"went\"\t\n",
       "\"from\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"the\"\t\n",
       "\"bathroom\"\t\n",
       "\"to\"\t\n",
       "\"the\"\t\n",
       "\"the\"\t\n",
       "\"bedsit\"\t\n",
       "\"because\"\t\n",
       "\"he\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"can't\"\t\n",
       "\"stand\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"billy.\"\t\n",
       "\"where\"\t\n",
       "\"is\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"the\"\t\n",
       "\"the\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"radio\"\t\n",
       "\"?\"\t\n"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "------------------ loading the first element of the bash  -------------------\n",
    "-- Using the first element of training set to test empirically if the model is working\n",
    "\n",
    "local tt = tensor_to_table(x)\n",
    "local ttt = table_to_string(tt)\n",
    "seed_text = tt\n",
    "len = #tt\n",
    "\n",
    "\n",
    "------------------ Computing predictions (log probabilities at each timestep) -------------------\n",
    "-- \n",
    "\n",
    "protos.rnn:evaluate()\n",
    "-- local current_state\n",
    "current_state = {}\n",
    "for L = 1, NUM_LAYERS do\n",
    "    -- c and h for all layers\n",
    "    local h_init = torch.zeros(1,RNN_SIZE):double()\n",
    "    table.insert(current_state,h_init:clone())\n",
    "    table.insert(current_state,h_init:clone())\n",
    "end\n",
    "state_size = #current_state\n",
    "\n",
    "-- do a few seeded timesteps\n",
    "if len > 0 then\n",
    "    -- print('seeding with '.. seed_text)\n",
    "    print('-----------------------')\n",
    "    for k,w in pairs(tt) do\n",
    "        print('\"'..w..'\"')\n",
    "        prev_word = torch.Tensor{voc[w]}\n",
    "        local lst = protos.rnn:forward{prev_word, unpack(current_state)}\n",
    "        -- lst is a list of [state1,state2,..stateN,output]. We want everything but last piece\n",
    "        current_state = {}\n",
    "        for i=1, state_size do table.insert(current_state, lst[i]) end\n",
    "        prediction = lst[#lst] -- last element holds the log probabilities\n",
    "    end    \n",
    "else\n",
    "    print('please add some seeding text')\n",
    "end\n",
    "\n",
    "------------------ samapling / argmaxing over the log probabilities at each timestep -------------------\n",
    "--\n",
    "\n",
    "-- start sampling / argmaxing\n",
    "for i=1, 5 do\n",
    "    -- log probabilities from the previous timestep\n",
    "    if TEMP_SAMPLING == 0 then\n",
    "        -- use argmax \n",
    "        local _, prev_word_ = prediction:max(2)\n",
    "        prev_word = prev_word_:resize(1)\n",
    "    else\n",
    "        -- use sampling\n",
    "        prediction:div(TEMP) -- scale by temperatrue\n",
    "        local probs = torch.exp(prediction):squeeze()\n",
    "        probs:div(torch.sum(probs)) -- renormalize so probs sum to one\n",
    "        prev_word = torch.multinomial(probs:float(), 1):resize(1):float()\n",
    "    end\n",
    "    -- forward the nn for next word\n",
    "    local lst = protos.rnn:forward{prev_word, unpack(current_state)}\n",
    "    current_state = {}\n",
    "    for i=1,state_size do table.insert(current_state, lst[i]) end\n",
    "    prediction = lst[#lst] -- last element holds the log probs\n",
    "    print(index[prev_word[1]])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T12:14:53",
     "start_time": "2017-08-28T10:14:53.121Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14\t\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor{voc['billy.']}\n",
    "print(voc['billy.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-28T11:51:27",
     "start_time": "2017-08-28T09:51:27.660Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "none",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "651px",
   "left": "0px",
   "right": "20px",
   "top": "131px",
   "width": "100px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
