{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turning data into one dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T11:47:04",
     "start_time": "2017-06-07T09:47:04.543Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:04",
     "start_time": "2017-06-07T11:55:04.294Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WordSPlitterMinibatchLoader = {}\n",
    "WordSPlitterMinibatchLoader.__index = WordSPlitterMinibatchLoader\n",
    "\n",
    "data_dir = \"/Users/david/Documents/MemoryNetwork/output_lua\"\n",
    "\n",
    "function WordSPlitterMinibatchLoader.create_vocabulary(input_file,vocab_file)\n",
    "    \tprint('loading text file....')\n",
    "\tlocal rawdata\n",
    "\tlocal tot_len = 0\n",
    "\tlocal f = assert(io.open(input_file,\"r\"))\n",
    "\tlocal max_sent_len = 0\n",
    "\tlocal sent_count = 0\n",
    "\t-- Create vocabulary if it doesn't exist yet\n",
    "\tprint('creating vocabulary mapping')\n",
    "\tlocal unordered = {}\n",
    "\trawdata = f:read():lower()\n",
    "\trepeat\n",
    "\t\tsent_count = sent_count + 1\n",
    "\t\tfor k,word in pairs(rawdata:split(\" \")) do \n",
    "\t\t\tword=word:lower()\n",
    "\t\t\tif not unordered[word] then unordered[word] = true end\n",
    "\t\tend\n",
    "\t\tsent_len = #rawdata:split(\" \")\n",
    "\t\tif sent_len > max_sent_len then max_sent_len=sent_len end\n",
    "\t\ttot_len = tot_len + sent_len\n",
    "\t\trawdata = f:read()\n",
    "\tuntil not rawdata\n",
    "\tf:close()\n",
    "\t-- sort into a table (i.e. keys become 1..N)\n",
    "\tlocal ordered = {}\n",
    "\tfor word in pairs(unordered) do ordered[#ordered + 1] = word end\n",
    "\ttable.sort(ordered)\n",
    "\t-- invert `ordered` to create the char->int mapping\n",
    "\tlocal vocab_mapping = {}\n",
    "\tfor i, word in ipairs(ordered) do\n",
    "\t\tvocab_mapping[word] = i\n",
    "\tend\n",
    "\tprint('saving ' .. vocab_file)\n",
    "    torch.save(vocab_file, vocab_mapping)\n",
    "    return {sent_count,vocab_mapping,max_sent_len}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:04",
     "start_time": "2017-06-07T11:55:04.726Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function iterate_trough(data,currline,rawdata,f)\n",
    "\trepeat\n",
    "\t\tfor k,word in pairs(rawdata:split(\" \")) do \n",
    "\t\t\tdata[{currline,k}] = vocab_mapping[word:lower()]\n",
    "\t\tend\n",
    "\t\tcurrline = currline + 1\n",
    "\t\trawdata = f:read():lower()\n",
    "\tuntil not rawdata\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:18",
     "start_time": "2017-06-07T11:55:18.626Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function WordSPlitterMinibatchLoader.create_tensor(sent_count,vocab_mapping,max_sent_len,input_file,tensor_file)\n",
    "\tprint('putting data into tensor...')\n",
    "\tlocal data = torch.ByteTensor(sent_count,max_sent_len):zero() -- store it into 1D first, then rearrange\n",
    "\tf = assert(io.open(input_file,\"r\"))\n",
    "\tlocal currline = 1\n",
    "\t-------- Writing in the tensor file \n",
    "\trawdata = f:read()\n",
    "\trepeat\n",
    "        rawdata = rawdata:lower()\n",
    "\t\tfor k,word in pairs(rawdata:split(\" \")) do \n",
    "\t\t\tdata[{currline,k}] = vocab_mapping[word:lower()]\n",
    "\t\tend\n",
    "\t\tcurrline = currline + 1\n",
    "\t\trawdata = f:read()\n",
    "\tuntil not rawdata\n",
    "\tf:close()\n",
    "\t-- save output preprocessed files\n",
    "    print('saving ' .. tensor_file)\n",
    "    torch.save(tensor_file, data)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:19",
     "start_time": "2017-06-07T11:55:19.113Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function WordSPlitterMinibatchLoader.text_to_tensor(input_file, out_vocab_file, out_tensor_file)\n",
    "    local timer = torch.Timer()\n",
    "    res = WordSPlitterMinibatchLoader.create_vocabulary(input_file,out_vocab_file)\n",
    "    local sent_count = res[1]\n",
    "    local vocab_mapping = res[2]\n",
    "    local max_sent_len = res[3]\n",
    "    return WordSPlitterMinibatchLoader.create_tensor(sent_count,vocab_mapping,max_sent_len,input_file,out_tensor_file)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:19",
     "start_time": "2017-06-07T11:55:19.529Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loading text file....\t\n",
       "creating vocabulary mapping\t\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "saving /Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\t\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "putting data into tensor...\t\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "saving /Users/david/Documents/MemoryNetwork/output_lua/data.t7\t\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = \"/Users/david/Documents/MemoryNetwork/preprocessing/output.txt\"\n",
    "out_vocab_file = \"/Users/david/Documents/MemoryNetwork/output_lua/vocab.t7\"\n",
    "out_tensor_file = \"/Users/david/Documents/MemoryNetwork/output_lua/data.t7\"\n",
    "WordSPlitterMinibatchLoader.text_to_tensor(input_file,out_vocab_file,out_tensor_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-07T13:55:58",
     "start_time": "2017-06-07T11:55:58.314Z"
    }
   },
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
